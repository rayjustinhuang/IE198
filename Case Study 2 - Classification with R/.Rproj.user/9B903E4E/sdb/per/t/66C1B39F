{
    "collab_server" : "",
    "contents" : "# Import libraries\nlibrary(dplyr)\nlibrary(RWeka)\nlibrary(ggplot2)\nlibrary(ROSE)\nlibrary(GGally)\nlibrary(xlsx)\n\n# Import data\norigtrain <- read.csv(\"ChurnTrain.csv\")\norigtest <- read.csv(\"ChurnTest.csv\")\n\n# View summary of data\nsummary(train)\n\n# Convert area code to categorical variable\norigtrain$Area.Code <- as.factor(origtrain$Area.Code)\norigtest$Area.Code <- as.factor(origtest$Area.Code)\n\n# Measure information gain to aid in feature selection\nInfoGain <- InfoGainAttributeEval(Churn. ~ ., data = train)\nInfoGain\n\n# Attributes with the lowest information gain are Account Length, Day Calls, Eve Calls, Night info (mins, call, charge)\n# The most important attributes are Day Charge, Customer Service Calls, and International Plan\n# I suspect that Minutes and Charge are collinear. Maybe remove one of them later after more preprocessing\ncolumnstodrop1 <- c(\"Account.Length\",\"Day.Calls\",\"Eve.Calls\",\"Night.Mins\",\"Night.Calls\",\"Night.Charge\")\n\n# We create new datasets with low-gain attributes removed\ntrain1 <- origtrain[ , !(names(origtrain) %in% columnstodrop1)]\ntest1 <- origtest[ , !(names(origtest) %in% columnstodrop1)]\n\nInfoGain2 <- InfoGainAttributeEval(Churn. ~ ., data = train1)\nInfoGain2\n\n# Geography also seems to have a minimal impact; we can remove this from the datasets\ntrain2 <- subset(train1, select = -c(Area.Code))\ntest2 <- subset(test1, select = -c(Area.Code))\n\nInfoGain3 <- InfoGainAttributeEval(Churn. ~ ., data = train2)\nInfoGain3\n\n# Check scatterplot matrix\nggscatmat(test2, columns = 1:ncol(test2)-1)\n\n# As suspected, Charge and Minutes are perfectly correlated. We remove minutes\ndropminutes <- c(\"Day.Mins\",\"Eve.Mins\",\"Intl.Mins\")\ntrain3 <- train2[ , !(names(train2) %in% dropminutes)]\ntest3 <- test2[ , !(names(test2) %in% dropminutes)]\n\n# Some of the data also appears to be normally distributed. We can standardize continuous variables\ncontinuousvars <- c(\"Day.Charge\", \"Eve.Charge\",\"Intl.Charge\")\nstdtrain <- train3 %>% mutate_each_(funs(scale(.) %>% as.vector), \n                             vars = continuousvars)\nstdtest <- test3 %>% mutate_each_(funs(scale(.) %>% as.vector), \n                                   vars = continuousvars)\n\n# Set seed\nset.seed(4747)\n\n# Model with a decision tree via the J48 algorithm to get a baseline\nInitialJ48Model <- J48(Churn. ~ ., data = stdtrain)\nInitialJ48Model\nsummary(InitialJ48Model)\n\n# Use 10-fold cross-validation and plot ROC curve\nevaluate_Weka_classifier(InitialJ48Model, numFolds = 10, class = TRUE, seed = 4747, complexity = TRUE)\nroc.curve(stdtrain$Churn., predict(InitialJ48Model))\n\n# AUC of 0.889; Recall of 0.720; Accuracy of 94.5994%\n\n# Ready to begin modeling\n\n# Clean up workspace\nrm(list = c(\"columnstodrop1\",\"continuousvars\",\"dropminutes\",\"InfoGain2\",\"InfoGain3\",\"origtest\",\"origtrain\",\n            \"test1\",\"test2\",\"test3\",\"train1\",\"train2\",\"train3\"))\n\n#############################\n\n# For better model evaluation, we should split train data into its own train and test sets \n\n# Split shuffled train data into further train and test sets\ntrainsize <- floor((nrow(stdtest)/(nrow(stdtrain)+nrow(stdtest)) * nrow(stdtrain)))\nsubtestrows <- sample(seq_len(nrow(stdtrain)), size = trainsize)\nsubtraining <- stdtrain[-subtestrows, ]\nholdout <- stdtrain[subtestrows, ]\n\n# Check if both training and holdout have the same percentage of churn entries\nprop.table(summary(subtraining$Churn.))\nprop.table(summary(holdout$Churn.))\n\n#############################\n\n# Check J48 Model with subtrain data for comparison\nJ48Modelsubtrainingdata <- J48(Churn. ~ ., data = subtraining)\nJ48Modelsubtrainingdata\nsummary(J48Modelsubtrainingdata)\nevaluate_Weka_classifier(J48Modelsubtrainingdata, numFolds = 10, class = TRUE, seed = 4747)\nroc.curve(subtraining$Churn., predict(J48Modelsubtrainingdata))\n\n# AUC of 0.895; Recall of 0.728; Accuracy of 94.8111%\n\n# Check J48 model learned on subtraining data with holdout data\nevaluate_Weka_classifier(J48Modelsubtrainingdata, newdata = holdout, numFolds = 10, class=TRUE,\n                         seed = 4747)\nroc.curve(holdout$Churn., predict(J48Modelsubtrainingdata, newdata = holdout))\n\n# AUC of 0.801; Recall of 0.569; Accuracy of 90.8019%\n\n# Area under the curve and Recall had a fairly significant drop\n# Correcting for class imbalance might improve the metrics\n\n#############################\n\n# Generate synthetic data via Randomly Over Sampling Examples to correct class imbalance\nsubtrainingROSE <- ROSE(Churn. ~ ., data = subtraining, seed = 4747)$data\nsummary(subtrainingROSE$Churn.)\n\n# NOTE: For churn data, Recall (\"True\" TPR) is most important; \"False\" TPR and accuracy do not need to be super high\n# AUC is always important (perhaps most important) to determine discriminatory power of the model\n\n# It is cheaper to retain customers than to have them switch -> better to be safe (predict more \n# people are going to churn to ensure we cover all people that might churn) than sorry\n\n#############################\n\n# Check J48 Model with ROSE-ed data for comparison\nJ48ModelROSEdata <- J48(Churn. ~ ., data = subtrainingROSE)\nJ48ModelROSEdata\nsummary(J48ModelROSEdata)\nevaluate_Weka_classifier(J48ModelROSEdata, numFolds = 10, class = TRUE, seed = 4747)\nroc.curve(subtrainingROSE$Churn., predict(J48ModelROSEdata))\n\n# AUC of 0.856; Recall of 0.782; Accuracy of 79.8257%\n\n# Check J48 model learned on ROSE-ed data with unseen holdout data\nevaluate_Weka_classifier(J48ModelROSEdata, newdata = holdout, numFolds = 10, class=TRUE,\n                         seed = 4747)\nroc.curve(holdout$Churn., predict(J48ModelROSEdata, newdata = holdout))\n\n# AUC of 0.781; Recall of 0.569; Accuracy of 90.8019%\n\n#############################\n\n# Metrics don't seem to have been hurt; but churn now has better representation in the dataset\n# Let's proceed with the ROSE-ed data\n\n# Model with a Random Forest\nRF <- make_Weka_classifier(\"weka/classifiers/trees/RandomForest\")\nRandomForest <- RF(Churn. ~ ., data = subtrainingROSE)\nevaluate_Weka_classifier(RandomForest, numFolds = 10, class = TRUE, complexity = TRUE, \n                         seed = 4747)\nroc.curve(subtrainingROSE$Churn., predict(RandomForest))\n\n# AUC of 1.000; Recall of 0.779; Accuracy of 82.3163%\n\n# Test on holdout data\nevaluate_Weka_classifier(RandomForest, newdata = holdout, numFolds = 10, class = TRUE, \n                         complexity = TRUE, seed = 4747)\nroc.curve(holdout$Churn., predict(RandomForest, newdata = holdout))\n\n# AUC of 0.827; Recall of 0.492; Accuracy of 89.6226%\n# Poor recall! Decent AUC.\n\n#############################\n\n# Try oversampling instead of ROSE\nsubtrainingOV <-  ovun.sample(Churn. ~ ., data = subtraining, method = \"over\")$data\nsummary(subtrainingOV$Churn.)\n\n# Model with J48 and Random Forest for comparison\nJ48ModelOVdata <- J48(Churn. ~ ., data = subtrainingOV)\nevaluate_Weka_classifier(J48ModelOVdata, numFolds = 10, class = TRUE, seed = 4747)\nroc.curve(subtrainingOV$Churn., predict(J48ModelOVdata))\n\n# AUC of 0.992; Recall of 0.993; Accuracy of 96.1687%\n\n# Check J48 model learned on oversampled data with unseen holdout data\nevaluate_Weka_classifier(J48ModelOVdata, newdata = holdout, numFolds = 10, class=TRUE,\n                         seed = 4747)\nroc.curve(holdout$Churn., predict(J48ModelOVdata, newdata = holdout))\n\n# AUC of 0.806; Recall of 0.569; Accuracy of 90.8019%\n\nRandomForestOV <- RF(Churn. ~ ., data = subtrainingOV)\nevaluate_Weka_classifier(RandomForest, numFolds = 10, class = TRUE, complexity = TRUE, \n                         seed = 4747)\nroc.curve(subtrainingOV$Churn., predict(RandomForestOV))\n\n# AUC of 1.000; Recall of 0.779; Accuracy of 82.3163%\n\n# Test on holdout data\nevaluate_Weka_classifier(RandomForestOV, newdata = holdout, numFolds = 10, class = TRUE, \n                         complexity = TRUE, seed = 4747)\nroc.curve(holdout$Churn., predict(RandomForestOV, newdata = holdout))\n\n# AUC of 0.835; Recall of 0.492; Accuracy of 89.6226%\n# Poor recall again. Try different models\n\n# Decision tree and random forest have almost identical performance with oversampled data\n# As expected, rules are largely the same. Try different models\n# Use oversampled data from here on out\n\n#############################\n\n# AdaBoosted Random Forest\nAdaBoostedRF <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                           control = Weka_control(W=list(RF)))\nsummary(AdaBoostedRF)\nroc.curve(subtrainingOV$Churn., predict(AdaBoostedRF))\nevaluate_Weka_classifier(AdaBoostedRF, complexity = TRUE,\n                         numFolds = 10, class=TRUE, seed = 4747)\n\n# AUC of 1.000; Recall of 0.999; Accuracy of 98.7711%\n# Good - but boosting may have overfit to training data\n\n# Test the model on our unseen holdout dataset\nevaluate_Weka_classifier(AdaBoostedRF, newdata = holdout,\n                         numFolds = 10, class=TRUE, seed = 4747)\nFirstPredictions <- predict(AdaBoostedRF, newdata = holdout)\nholdoutpredictions <- data.frame(FirstPredictions)\nroc.curve(holdout$Churn., holdoutpredictions$FirstPredictions)\n\n# AUC of 0.834; Recall of 0.492; Accuracy of 90.3302%\n# Still bad recall...\n\n# Support Vector Machine-based Classifier\nSVC <- SMO(Churn. ~ ., data = subtrainingOV)\nevaluate_Weka_classifier(SVC, complexity = TRUE, numFolds = 10, class = TRUE,\n                         seed = 4747)\nroc.curve(subtrainingOV$Churn., predict(SVC))\n\n# AUC of 0.768; Recall of 0.784; Accuracy of 76.8193%\n\nevaluate_Weka_classifier(SVC, newdata = holdout, complexity = TRUE, numFolds = 10, class = TRUE,\n                         seed = 4747)\nroc.curve(holdout$Churn., predict(SVC, newdata = holdout))\n\n# Recall of 0! Bad\n\n# Artificial Neural Network\nMLP <- make_Weka_classifier(\"weka/classifiers/functions/MultilayerPerceptron\")\nANN <- MLP(Churn. ~ ., data = subtrainingOV)\nevaluate_Weka_classifier(ANN, numFolds = 10, class=TRUE, seed=4747)\nroc.curve(subtrainingOV$Churn., predict(ANN))\n\n# AUC of 0.897; Recall of 0.843; Accuracy of 88.3373%\n\nevaluate_Weka_classifier(ANN, newdata = holdout, complexity = TRUE,\n                         numFolds = 10, class=TRUE, seed=4747)\nholdoutpredictions$ANNpredictions <- predict(ANN, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$ANNpredictions)\n\n# AUC of 0.841; True TPR of 0.538; Accuracy of 90.0943%\n# Second best model after J48. Try boosting both\n\n# AdaBoosted J48 decision tree\nAdaBoostedJ48 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                           control = Weka_control(W=list(J48)))\nroc.curve(subtrainingOV$Churn., predict(AdaBoostedJ48))\nevaluate_Weka_classifier(AdaBoostedJ48, complexity = TRUE,\n                         numFolds = 10, class=TRUE, seed = 4747)\n\n# AUC of 1.000; Recall of 0.999; Accuracy of 98.6506%\n\n# Test the model on our unseen holdout dataset\nevaluate_Weka_classifier(AdaBoostedJ48, newdata = holdout,\n                         numFolds = 10, class=TRUE, seed = 4747)\nholdoutpredictions$BoostedJ48predictions <- predict(AdaBoostedJ48, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$BoostedJ48predictions)\n\n# AUC of 0.802; Recall of 0.600; Accuracy of 91.2736%\n# Best recall so far!\n\n# AdaBoosted Artificial Neural Network\nAdaBoostedANN <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                            control = Weka_control(W=list(MLP)))\nroc.curve(subtrainingOV$Churn., predict(AdaBoostedANN))\nevaluate_Weka_classifier(AdaBoostedANN, complexity = TRUE,\n                         numFolds = 10, class = TRUE, seed = 4747)\n\n# AUC of 0.919; Recall of 0.862; Accuracy of 90.0723%\n\n# Holdout data\nevaluate_Weka_classifier(AdaBoostedANN, newdata = holdout,\n                         numFolds = 10, class = TRUE, seed = 4747)\nholdoutpredictions$BoostedANNpredictions <- predict(AdaBoostedANN, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$BoostedANNpredictions)\n\n# AUC of 0.850; Recall of 0.538; Accuracy of 89.6226%\n# Best AUC so far!\n\n# Try kNN with regular subtraining data (not oversampled)\nkNN <- IBk(Churn. ~ ., data = subtraining)\nroc.curve(subtraining$Churn., predict(kNN))\nevaluate_Weka_classifier(kNN, complexity = TRUE, numFolds = 10, class = TRUE,\n                         seed = 4747)\n\n# AUC of 1.000; Recall of 0.571; Accuracy of 89.6226%\n\nevaluate_Weka_classifier(kNN, newdata = holdout, numFolds = 10,\n                         class = TRUE, seed = 4747)\nholdoutpredictions$kNNpredictions <- predict(kNN, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$kNNpredictions)\n\n# AUC of 0.715; Recall of 0.431; Accuracy of 83.9623%\n\n# Try boosted decision stumps\nAdaBoostedStumps <- AdaBoostM1(Churn. ~ ., data = subtrainingOV)\nroc.curve(subtrainingOV$Churn., predict(AdaBoostedStumps))\nevaluate_Weka_classifier(AdaBoostedStumps, complexity = TRUE, numFolds = 10, class = TRUE,\n                         seed = 4747)\n\n# AUC of 0.772; Recall of 0.807; Accuracy of 80.6024%\nevaluate_Weka_classifier(AdaBoostedStumps, newdata = holdout, numFolds = 10,\n                         class = TRUE, seed = 4747)\nholdoutpredictions$BoostedStumppredictions <- predict(AdaBoostedStumps, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$BoostedStumppredictions)\n\n# AUC of 0.745; Recall of 0.369; Accuracy of 88.2075%\n\n#############################\n\n# Time to narrow down and fine-tune the final model.\n\n# Results of three best models on holdout data\n# Boosted Forest: AUC of 0.834; Recall of 0.492; Accuracy of 90.3302%\n# Boosted Tree: AUC of 0.802; Recall of 0.600; Accuracy of 91.2736%\n# Boosted ANN: AUC of 0.850; Recall of 0.538; Accuracy of 89.6226%\n\n# The most important features are AUC, Recall, and Accuracy in that order\n# Best balance of all is the Boosted ANN - Winner\n\n# Test if using ROSE-ed data improves the model\n\n# AdaBoosted Artificial Neural Network with ROSE-ed data\nAdaBoostedANN.ROSE <- AdaBoostM1(Churn. ~ ., data = subtrainingROSE,\n                            control = Weka_control(W=list(MLP)))\nroc.curve(subtrainingROSE$Churn., predict(AdaBoostedANN.ROSE))\nevaluate_Weka_classifier(AdaBoostedANN.ROSE, complexity = TRUE,\n                         numFolds = 10, class = TRUE, seed = 4747)\n\n# AUC of 0.837; Recall of 0.791; Accuracy of 81.9012%\n\n# Holdout data\nevaluate_Weka_classifier(AdaBoostedANN.ROSE, newdata = holdout,\n                         numFolds = 10, class = TRUE, seed = 4747)\nholdoutpredictions$BoostedANN.ROSEpredictions <- predict(AdaBoostedANN.ROSE, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$BoostedANN.ROSEpredictions)\n\n# AUC of 0.810; Recall of 0.538; Accuracy of 89.6226%\n\n# Test for the effect of using regular data on the model\nAdaBoostedANN.vanilla <- AdaBoostM1(Churn. ~ ., data = subtraining,\n                               control = Weka_control(W=list(MLP)))\nroc.curve(subtraining$Churn., predict(AdaBoostedANN.vanilla))\nevaluate_Weka_classifier(AdaBoostedANN.vanilla, complexity = TRUE,\n                         numFolds = 10, class = TRUE, seed = 4747)\n\n# AUC of 0.897; Recall of 0.629; Accuracy of 91.9884%\n\n# Holdout set\nevaluate_Weka_classifier(AdaBoostedANN.vanilla, newdata = holdout,\n                         numFolds = 10, class = TRUE, seed = 4747)\nholdoutpredictions$BoostedANN.vanillapredictions <- predict(AdaBoostedANN.vanilla, newdata = holdout)\nroc.curve(holdout$Churn., holdoutpredictions$BoostedANN.vanillapredictions)\n\n# AUC of 0.813; Recall of 0.538; Accuracy of 89.6226%\n\n# The best results happened with oversampled data; we will use oversampled data to train our final model\n\n#############################\n\n# We now tune the parameters of the Boosted ANN\n\n# Artificial Neural Network - tuning for no. of epochs\nAdaBoostedANNepochs50 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                            control = Weka_control(W = list(MLP, L = 0.15, N = 50)))\nAdaBoostedANNepochs75 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                    control = Weka_control(W=list(MLP, L = 0.15, N = 75)))\nAdaBoostedANNepochs100 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                    control = Weka_control(W=list(MLP, L = 0.15, N = 100)))\nAdaBoostedANNepochs150 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                    control = Weka_control(W=list(MLP, L = 0.15, N = 150)))\nAdaBoostedANNepochs250 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                    control = Weka_control(W=list(MLP, L = 0.15, N = 250)))\nAdaBoostedANNepochs350 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.15, N = 350)))\nAdaBoostedANNepochs450 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.15, N = 450)))\nAdaBoostedANNepochs500 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.15, N = 500)))\nAdaBoostedANNepochs550 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.15, N = 550)))\nAdaBoostedANNepochs1000 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.15, N = 1000)))\n\n\nsummary(AdaBoostedANNepochs50, newdata = holdout, class = TRUE)   # Recall = 0.769; Accuracy = 89.8585%\nsummary(AdaBoostedANNepochs75, newdata = holdout, class = TRUE)   # Recall = 0.785; Accuracy = 90.566%\nsummary(AdaBoostedANNepochs100, newdata = holdout, class = TRUE)  # Recall = 0.769; Accuracy = 89.1509%\nsummary(AdaBoostedANNepochs150, newdata = holdout, class = TRUE)  # Recall = 0.754; Accuracy = 90.3302%\nsummary(AdaBoostedANNepochs250, newdata = holdout, class = TRUE)  # Recall = 0.874; Accuracy = 89.6226%\nsummary(AdaBoostedANNepochs350, newdata = holdout, class = TRUE)  # Recall = 0.769; Accuracy = 89.6226%\nsummary(AdaBoostedANNepochs450, newdata = holdout, class = TRUE)  # Recall = 0.754; Accuracy = 89.6226%\nsummary(AdaBoostedANNepochs500, newdata = holdout, class = TRUE)  # Recall = 0.738; Accuracy = 89.8585%\nsummary(AdaBoostedANNepochs550, newdata = holdout, class = TRUE)  # Recall = 0.738; Accuracy = 89.3868%\nsummary(AdaBoostedANNepochs1000, newdata = holdout, class = TRUE)  # Recall = 0.754; Accuracy = 90.0943%\n\n# 250 is a good number of epochs: Recall = 0.874; Accuracy = 89.6226%\n# Tune for learning rate\n\nAdaBoostedANNl0.1 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                     control = Weka_control(W=list(MLP, L = 0.10, N = 250)))\nAdaBoostedANNl0.2 <- AdaBoostM1(Churn. ~ ., data = subtrainingOV,\n                                control = Weka_control(W=list(MLP, L = 0.20, N = 250)))\n\nsummary(AdaBoostedANNl0.1, newdata = holdout, class = TRUE)  # Recall = 0.710; Accuracy = 90.566%\nsummary(AdaBoostedANNl0.2, newdata = holdout, class = TRUE)  # Recall = 0.754; Accuracy = 88.6792%\n\n# 0.15 is a good learning rate. Testing on the holdout data:\n\nevaluate_Weka_classifier(AdaBoostedANNepochs250, newdata = holdout, class = TRUE,\n                         seed = 4747, numFolds = 10)\nroc.curve(holdout$Churn., predict(AdaBoostedANNepochs250, newdata = holdout))\n\n# AUC of 0.832; Recall of 0.554; Accuracy of 88.4434%\n\n#############################\n\n# Preparing the final model\n\n# Use oversampling on the stdtrain data\nfinaltrain <-  ovun.sample(Churn. ~ ., data = stdtrain, method = \"over\")$data\n\nFinalModel <- AdaBoostM1(Churn. ~ ., data = finaltrain,\n                          control = Weka_control(W=list(MLP, L = 0.15, N = 250)))\nevaluate_Weka_classifier(FinalModel, class = TRUE, seed = 4747, numFolds = 10)\nroc.curve(finaltrain$Churn., predict(FinalModel))\n\n# Create final predictions on test data\nFinalPredictions <- predict(FinalModel, newdata = stdtest)\nsummary(FinalPredictions)\n\n# Write output Excel file\nwrite.xlsx(FinalPredictions, \"IE 198 Case 2 - Huang.xlsx\")\nwrite.csv(FinalPredictions, \"IE 198 Case 2 - Huang.csv\")\n",
    "created" : 1490198598708.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1868405",
    "id" : "66C1B39F",
    "lastKnownWriteTime" : 1490242515,
    "last_content_update" : 1490349616200,
    "path" : "~/UP Files/IE 198/IE 198 Work/Case Study 2 - Classification with R/CopyOfCase 2 - Classification with R (Final).R",
    "project_path" : "CopyOfCase 2 - Classification with R (Final).R",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}