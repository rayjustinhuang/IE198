{
    "collab_server" : "",
    "contents" : "##### Chapter 4: Classification using Naive Bayes --------------------\n\n## Example: Filtering spam SMS messages ----\n## Step 2: Exploring and preparing the data ---- \n\n# read the sms data into the sms data frame\nsms_raw <- read.csv(\"sms_spam.csv\", stringsAsFactors = FALSE)\n\n# examine the structure of the sms data\nstr(sms_raw)\n\n# convert spam/ham to factor.\nsms_raw$type <- factor(sms_raw$type)\n\n# examine the type variable more carefully\nstr(sms_raw$type)\ntable(sms_raw$type)\n\n# build a corpus using the text mining (tm) package\nlibrary(tm)\nsms_corpus <- VCorpus(VectorSource(sms_raw$text))\n\n# examine the sms corpus\nprint(sms_corpus)\ninspect(sms_corpus[1:2])\n\nas.character(sms_corpus[[1]])\nlapply(sms_corpus[1:2], as.character)\n\n# clean up the corpus using tm_map()\nsms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))\n\n# show the difference between sms_corpus and corpus_clean\nas.character(sms_corpus[[1]])\nas.character(sms_corpus_clean[[1]])\n\nsms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) # remove numbers\nsms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords()) # remove stop words\nsms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation) # remove punctuation\n\n# tip: create a custom function to replace (rather than remove) punctuation\nremovePunctuation(\"hello...world\")\nreplacePunctuation <- function(x) { gsub(\"[[:punct:]]+\", \" \", x) }\nreplacePunctuation(\"hello...world\")\n\n# illustration of word stemming\nlibrary(SnowballC)\nwordStem(c(\"learn\", \"learned\", \"learning\", \"learns\"))\n\nsms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)\n\nsms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace) # eliminate unneeded whitespace\n\n# examine the final clean corpus\nlapply(sms_corpus[1:3], as.character)\nlapply(sms_corpus_clean[1:3], as.character)\n\n# create a document-term sparse matrix\nsms_dtm <- DocumentTermMatrix(sms_corpus_clean)\n\n# alternative solution: create a document-term sparse matrix directly from the SMS corpus\nsms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(\n  tolower = TRUE,\n  removeNumbers = TRUE,\n  stopwords = TRUE,\n  removePunctuation = TRUE,\n  stemming = TRUE\n))\n\n# alternative solution: using custom stop words function ensures identical result\nsms_dtm3 <- DocumentTermMatrix(sms_corpus, control = list(\n  tolower = TRUE,\n  removeNumbers = TRUE,\n  stopwords = function(x) { removeWords(x, stopwords()) },\n  removePunctuation = TRUE,\n  stemming = TRUE\n))\n\n# compare the result\nsms_dtm\nsms_dtm2\nsms_dtm3\n\n# creating training and test datasets\nsms_dtm_train <- sms_dtm[1:4169, ]\nsms_dtm_test  <- sms_dtm[4170:5559, ]\n\n# also save the labels\nsms_train_labels <- sms_raw[1:4169, ]$type\nsms_test_labels  <- sms_raw[4170:5559, ]$type\n\n# check that the proportion of spam is similar\nprop.table(table(sms_train_labels))\nprop.table(table(sms_test_labels))\n\n# word cloud visualization\nlibrary(wordcloud)\nwordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)\n\n# subset the training data into spam and ham groups\nspam <- subset(sms_raw, type == \"spam\")\nham  <- subset(sms_raw, type == \"ham\")\n\nwordcloud(spam$text, max.words = 40, scale = c(3, 0.5))\nwordcloud(ham$text, max.words = 40, scale = c(3, 0.5))\n\nsms_dtm_freq_train <- removeSparseTerms(sms_dtm_train, 0.999)\nsms_dtm_freq_train\n\n# indicator features for frequent words\nfindFreqTerms(sms_dtm_train, 5)\n\n# save frequently-appearing terms to a character vector\nsms_freq_words <- findFreqTerms(sms_dtm_train, 5)\nstr(sms_freq_words)\n\n# create DTMs with only the frequent terms\nsms_dtm_freq_train <- sms_dtm_train[ , sms_freq_words]\nsms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]\n\n# convert counts to a factor\nconvert_counts <- function(x) {\n  x <- ifelse(x > 0, \"Yes\", \"No\")\n}\n\n# apply() convert_counts() to columns of train/test data\nsms_train <- apply(sms_dtm_freq_train, MARGIN = 2, convert_counts)\nsms_test  <- apply(sms_dtm_freq_test, MARGIN = 2, convert_counts)\n\n## Step 3: Training a model on the data ----\nlibrary(e1071)\nsms_classifier <- naiveBayes(sms_train, sms_train_labels)\n\n## Step 4: Evaluating model performance ----\nsms_test_pred <- predict(sms_classifier, sms_test)\n\nlibrary(gmodels)\nCrossTable(sms_test_pred, sms_test_labels,\n           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,\n           dnn = c('predicted', 'actual'))\n\n## Step 5: Improving model performance ----\nsms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace = 1)\nsms_test_pred2 <- predict(sms_classifier2, sms_test)\nCrossTable(sms_test_pred2, sms_test_labels,\n           prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,\n           dnn = c('predicted', 'actual'))\n",
    "created" : 1486789394315.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2351258494",
    "id" : "2F21F000",
    "lastKnownWriteTime" : 1438217488,
    "last_content_update" : 1438217488,
    "path" : "~/Books/Machine Learning with R/MLwR-master/Machine Learning with R (2nd Ed.)/Chapter 04/MLwR_v2_04.r",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 14,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}